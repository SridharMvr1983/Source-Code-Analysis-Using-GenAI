{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\GenerativeAI\\\\Source-Code-Analysis-Using-GenAI\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'c:\\\\GenerativeAI\\\\Source-Code-Analysis-Using-GenAI\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/entbappy/End-to-end-ML-Project-Implementation\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/src/mlProject',\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport sys\\nimport logging\\n\\nlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\\n\\nlog_dir = \"logs\"\\nlog_filepath = os.path.join(log_dir,\"running_logs.log\")\\nos.makedirs(log_dir, exist_ok=True)\\n\\n\\nlogging.basicConfig(\\n    level= logging.INFO,\\n    format= logging_str,\\n\\n    handlers=[\\n        logging.FileHandler(log_filepath),\\n        logging.StreamHandler(sys.stdout)\\n    ]\\n)\\n\\nlogger = logging.getLogger(\"mlProjectLogger\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport urllib.request as request\\nimport zipfile\\nfrom mlProject import logger\\nfrom mlProject.utils.common import get_size\\nfrom mlProject.entity.config_entity import DataIngestionConfig\\nfrom pathlib import Path\\n\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config = config\\n\\n    \\n    def download_file(self):\\n        if not os.path.exists(self.config.local_data_file):\\n            filename, headers = request.urlretrieve(\\n                url = self.config.source_URL,\\n                filename = self.config.local_data_file\\n            )\\n            logger.info(f\"{filename} download! with following info: \\\\n{headers}\")\\n        else:\\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\\n\\n    \\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom mlProject import logger\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataTransformationConfig\\n\\n\\nclass DataTransformation:\\n    def __init__(self, config: DataTransformationConfig):\\n        self.config = config\\n\\n    \\n    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\\n    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\\n\\n    # I am only adding train_test_spliting cz this data is already cleaned up\\n\\n\\n    def train_test_spliting(self):\\n        data = pd.read_csv(self.config.data_path)\\n\\n        # Split the data into training and test sets. (0.75, 0.25) split.\\n        train, test = train_test_split(data)\\n\\n        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\\n        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\\n\\n        logger.info(\"Splited data into training and test sets\")\\n        logger.info(train.shape)\\n        logger.info(test.shape)\\n\\n        print(train.shape)\\n        print(test.shape)\\n        ', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom mlProject import logger\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataValidationConfig\\n                                    \\n\\n\\n\\nclass DataValiadtion:\\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n\\n    def validate_all_columns(self)-> bool:\\n        try:\\n            validation_status = None\\n\\n            data = pd.read_csv(self.config.unzip_data_dir)\\n            all_cols = list(data.columns)\\n\\n            all_schema = self.config.all_schema.keys()\\n\\n            \\n            for col in all_cols:\\n                if col not in all_schema:\\n                    validation_status = False\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                else:\\n                    validation_status = True\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n\\n            return validation_status\\n        \\n        except Exception as e:\\n            raise e\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport pandas as pd\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom mlProject.utils.common import save_json\\nfrom urllib.parse import urlparse\\nimport numpy as np\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelEvaluationConfig\\nfrom pathlib import Path\\n\\n\\nclass ModelEvaluation:\\n    def __init__(self, config: ModelEvaluationConfig):\\n        self.config = config\\n\\n    \\n    def eval_metrics(self,actual, pred):\\n        rmse = np.sqrt(mean_squared_error(actual, pred))\\n        mae = mean_absolute_error(actual, pred)\\n        r2 = r2_score(actual, pred)\\n        return rmse, mae, r2\\n    \\n\\n\\n    def save_results(self):\\n\\n        test_data = pd.read_csv(self.config.test_data_path)\\n        model = joblib.load(self.config.model_path)\\n\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        test_y = test_data[[self.config.target_column]]\\n        \\n        predicted_qualities = model.predict(test_x)\\n\\n        (rmse, mae, r2) = self.eval_metrics(test_y, predicted_qualities)\\n        \\n        # Saving metrics as local\\n        scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\\n        save_json(path=Path(self.config.metric_file_name), data=scores)\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import pandas as pd\\nimport os\\nfrom mlProject import logger\\nfrom sklearn.linear_model import ElasticNet\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelTrainerConfig\\n\\n\\n\\nclass ModelTrainer:\\n    def __init__(self, config: ModelTrainerConfig):\\n        self.config = config\\n\\n    \\n    def train(self):\\n        train_data = pd.read_csv(self.config.train_data_path)\\n        test_data = pd.read_csv(self.config.test_data_path)\\n\\n\\n        train_x = train_data.drop([self.config.target_column], axis=1)\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        train_y = train_data[[self.config.target_column]]\\n        test_y = test_data[[self.config.target_column]]\\n\\n\\n        lr = ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio, random_state=42)\\n        lr.fit(train_x, train_y)\\n\\n        joblib.dump(lr, os.path.join(self.config.root_dir, self.config.model_name))\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.constants import *\\nfrom mlProject.utils.common import read_yaml, create_directories\\nfrom mlProject.entity.config_entity import (DataIngestionConfig,\\n                                            DataValidationConfig,\\n                                            DataTransformationConfig,\\n                                            ModelTrainerConfig,\\n                                            ModelEvaluationConfig)\\n\\nclass ConfigurationManager:\\n    def __init__(\\n        self,\\n        config_filepath = CONFIG_FILE_PATH,\\n        params_filepath = PARAMS_FILE_PATH,\\n        schema_filepath = SCHEMA_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n        self.schema = read_yaml(schema_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n        \\n\\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        config = self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n\\n        data_ingestion_config = DataIngestionConfig(\\n            root_dir=config.root_dir,\\n            source_URL=config.source_URL,\\n            local_data_file=config.local_data_file,\\n            unzip_dir=config.unzip_dir \\n        )\\n\\n        return data_ingestion_config\\n    \\n\\n    \\n    def get_data_validation_config(self) -> DataValidationConfig:\\n        config = self.config.data_validation\\n        schema = self.schema.COLUMNS\\n\\n        create_directories([config.root_dir])\\n\\n        data_validation_config = DataValidationConfig(\\n            root_dir=config.root_dir,\\n            STATUS_FILE=config.STATUS_FILE,\\n            unzip_data_dir = config.unzip_data_dir,\\n            all_schema=schema,\\n        )\\n\\n        return data_validation_config\\n    \\n\\n    \\n    def get_data_transformation_config(self) -> DataTransformationConfig:\\n        config = self.config.data_transformation\\n\\n        create_directories([config.root_dir])\\n\\n        data_transformation_config = DataTransformationConfig(\\n            root_dir=config.root_dir,\\n            data_path=config.data_path,\\n        )\\n\\n        return data_transformation_config\\n    \\n\\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\\n        config = self.config.model_trainer\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_trainer_config = ModelTrainerConfig(\\n            root_dir=config.root_dir,\\n            train_data_path = config.train_data_path,\\n            test_data_path = config.test_data_path,\\n            model_name = config.model_name,\\n            alpha = params.alpha,\\n            l1_ratio = params.l1_ratio,\\n            target_column = schema.name\\n            \\n        )\\n\\n        return model_trainer_config\\n    \\n\\n\\n    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\\n        config = self.config.model_evaluation\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_evaluation_config = ModelEvaluationConfig(\\n            root_dir=config.root_dir,\\n            test_data_path=config.test_data_path,\\n            model_path = config.model_path,\\n            all_params=params,\\n            metric_file_name = config.metric_file_name,\\n            target_column = schema.name\\n           \\n        )\\n\\n        return model_evaluation_config', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nSCHEMA_FILE_PATH = Path(\"schema.yaml\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\nfrom pathlib import Path\\n\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path\\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataValidationConfig:\\n    root_dir: Path\\n    STATUS_FILE: str\\n    unzip_data_dir: Path\\n    all_schema: dict\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataTransformationConfig:\\n    root_dir: Path\\n    data_path: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass ModelTrainerConfig:\\n    root_dir: Path\\n    train_data_path: Path\\n    test_data_path: Path\\n    model_name: str\\n    alpha: float\\n    l1_ratio: float\\n    target_column: str\\n\\n\\n@dataclass(frozen=True)\\nclass ModelEvaluationConfig:\\n    root_dir: Path\\n    test_data_path: Path\\n    model_path: Path\\n    all_params: dict\\n    metric_file_name: Path\\n    target_column: str', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"import joblib \\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.model = joblib.load(Path('artifacts/model_trainer/model.joblib'))\\n\\n    \\n    def predict(self, data):\\n        prediction = self.model.predict(data)\\n\\n        return prediction\", metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_ingestion import DataIngestion\\nfrom mlProject import logger\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\n\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config=data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_validation import DataValiadtion\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Data Validation stage\"\\n\\nclass DataValidationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_validation_config = config.get_data_validation_config()\\n        data_validation = DataValiadtion(config=data_validation_config)\\n        data_validation.validate_all_columns()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataValidationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_transformation import DataTransformation\\nfrom mlProject import logger\\nfrom pathlib import Path\\n\\n\\nSTAGE_NAME = \"Data Transformation stage\"\\n\\nclass DataTransformationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n\\n    def main(self):\\n        try:\\n            with open(Path(\"artifacts/data_validation/status.txt\"), \"r\") as f:\\n                status = f.read().split(\" \")[-1]\\n\\n            if status == \"True\":\\n                config = ConfigurationManager()\\n                data_transformation_config = config.get_data_transformation_config()\\n                data_transformation = DataTransformation(config=data_transformation_config)\\n                data_transformation.train_test_spliting()\\n\\n            else:\\n                raise Exception(\"You data schema is not valid\")\\n\\n        except Exception as e:\\n            print(e)\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_trainer import ModelTrainer\\nfrom mlProject import logger\\n\\n\\n\\nSTAGE_NAME = \"Model Trainer stage\"\\n\\nclass ModelTrainerTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_trainer_config = config.get_model_trainer_config()\\n        model_trainer_config = ModelTrainer(config=model_trainer_config)\\n        model_trainer_config.train()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelTrainerTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_evaluation import ModelEvaluation\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Model evaluation stage\"\\n\\nclass ModelEvaluationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_evaluation_config = config.get_model_evaluation_config()\\n        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\\n        model_evaluation_config.save_results()\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelEvaluationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_05_model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom box.exceptions import BoxValueError\\nimport yaml\\nfrom mlProject import logger\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\n\\n\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except BoxValueError:\\n        raise ValueError(\"yaml file is empty\")\\n    except Exception as e:\\n        raise e\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"\\n\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"***************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Knowledge base (vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['self', 'args', 'kwargs'])\nPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:592\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    591\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m    593\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    594\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    595\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    596\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    597\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    598\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    599\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    600\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    601\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    603\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:547\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[0;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \n\u001b[0;32m    530\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    548\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    549\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    550\u001b[0m         persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    551\u001b[0m         client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    552\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    553\u001b[0m         collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    554\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    555\u001b[0m     )\n\u001b[0;32m    556\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:115\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m         _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;129;01mor\u001b[39;00m persist_directory\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_relevance_score_fn \u001b[38;5;241m=\u001b[39m relevance_score_fn\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\chromadb\\api\\client.py:237\u001b[0m, in \u001b[0;36mClient.get_or_create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_collection\u001b[39m(\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\chromadb\\api\\segment.py:217\u001b[0m, in \u001b[0;36mSegmentAPI.get_or_create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader, tenant, database)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentAPI.get_or_create_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mOPERATION\n\u001b[0;32m    204\u001b[0m )\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[0;32m    216\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\chromadb\\api\\segment.py:191\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_product_telemetry_client\u001b[38;5;241m.\u001b[39mcapture(\n\u001b[0;32m    184\u001b[0m     ClientCreateCollectionEvent(\n\u001b[0;32m    185\u001b[0m         collection_uuid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m),\n\u001b[0;32m    186\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    188\u001b[0m )\n\u001b[0;32m    189\u001b[0m add_attributes_to_current_span({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_uuid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m)})\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:87\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, client, name, id, embedding_function, data_loader, tenant, database, metadata)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Check to make sure the embedding function has the right signature, as defined by the EmbeddingFunction protocol\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mvalidate_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;241m=\u001b[39m data_loader\n",
      "File \u001b[1;32mc:\\Users\\ZBOOK\\anaconda3\\envs\\lenv\\lib\\site-packages\\chromadb\\api\\types.py:211\u001b[0m, in \u001b[0;36mvalidate_embedding_function\u001b[1;34m(embedding_function)\u001b[0m\n\u001b[0;32m    208\u001b[0m protocol_signature \u001b[38;5;241m=\u001b[39m signature(EmbeddingFunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_signature \u001b[38;5;241m==\u001b[39m protocol_signature:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected EmbeddingFunction.__call__ to have the following signature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotocol_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['self', 'args', 'kwargs'])\nPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
